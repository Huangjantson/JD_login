{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Feature Generation(on Spark remote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__doc__', '__module__', 'data_path', 'feature_dict', 'feature_path', 'result_path', 'single_module_validation_indice_set', 'trade_train_size', 'train_2_6_index']\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    pass\n",
    "config = pd.read_pickle('config.pkl')\n",
    "print dir(config)\n",
    "\n",
    "data_path = '../../kaggleData/JD_logging/'\n",
    "feature_path = '../../kaggleData/JD_logging/features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "login_tt = pd.read_csv(data_path+'login_tt.csv')\n",
    "trade_tt = pd.read_csv(data_path+'trade_tt.csv')\n",
    "\n",
    "login_tt['time'] = login_tt['time'].apply(lambda x : datetime.datetime.strptime(x , '%Y-%m-%d %H:%M:%S'))\n",
    "trade_tt['time'] = trade_tt['time'].apply(lambda x : datetime.datetime.strptime(x , '%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_related_logins_before(row,login_table,*args,**kw):\n",
    "    related_logins = login_table[login_table['id'] == row.id]\n",
    "    related_logins_before = related_logins[related_logins['time']<row.time]\n",
    "    return related_logins_before    \n",
    "\n",
    "def find_related_recent_logins_within_days(row,login_table,days,*args,**kw):    \n",
    "    recent_logins = find_related_logins_before(row,login_table)\n",
    "    if len(recent_logins)>0:\n",
    "        recent_logins['from_now'] =  row.time - recent_logins['time']\n",
    "        return recent_logins[recent_logins['from_now']<datetime.timedelta(days = days)]\n",
    "    else:\n",
    "        recent_logins['from_now'] = np.nan\n",
    "        return recent_logins[recent_logins['from_now']<datetime.timedelta(days = days)]\n",
    "        \n",
    "\n",
    "def find_related_trades_before(row,trade_table,*args,**kw):\n",
    "    related_trades = trade_table[trade_table['id'] == row.id]\n",
    "    related_trades_before = related_trades[related_trades['time']<row.time]\n",
    "    return related_trades_before    \n",
    "\n",
    "def find_related_recent_trades_within_days(row,trade_table,days,*args,**kw):    \n",
    "    recent_trades = find_related_trades_before(row,trade_table)\n",
    "    if len(recent_trades)>0:\n",
    "        recent_trades['from_now'] =  row.time - recent_trades['time']\n",
    "        return recent_trades[recent_trades['from_now']<datetime.timedelta(days = days)]  \n",
    "    else:\n",
    "        recent_trades['from_now'] = np.nan\n",
    "        return recent_trades[recent_trades['from_now']<datetime.timedelta(days = days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_multiple_feature_dicts_wihtin_days(row,login_table,trade_table):\n",
    "    date_range = [360,30,15,7,3,1]\n",
    "    result_dict = {}\n",
    "    for days in date_range:\n",
    "        if days ==360:\n",
    "            recent_trade_table = find_related_recent_trades_within_days(row,trade_table,days)\n",
    "            recent_login_table = find_related_recent_logins_within_days(row,login_table,days)\n",
    "        else:\n",
    "            recent_trade_table = find_related_recent_trades_within_days(row,recent_trade_table,days)\n",
    "            recent_login_table = find_related_recent_logins_within_days(row,recent_login_table,days)\n",
    "        \n",
    "        result_dict[days] = build_statistical_feature_dict(recent_login_table,recent_trade_table)    \n",
    "    return result_dict\n",
    "\n",
    "def build_statistical_feature_dict(recent_login_table,recent_trade_table,*args,**kw):\n",
    "    \"\"\"\n",
    "    ID交易次数\n",
    "    #最近的前一次交易时间 - 在顶层使用\n",
    "    ID登录次数\n",
    "    交易/登录次数比\n",
    "    ID登录成功次数（大于零的项）\n",
    "    ID登录失败次数（小于零的项）\n",
    "    ID登录成功比率\n",
    "    交易/成功登录次数比，交易/失败次数比\n",
    "    是否有连续login失败\n",
    "    login失败到下一次尝试的平均时间、最大时间、最小时间、时间中位数、方差\n",
    "    timelong平均值，最大值，最小值，中位数，方差\n",
    "    timelong方差（仅一个时为0或N/A）\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    \n",
    "    trade_times = len(recent_trade_table)\n",
    "    login_times = len(recent_login_table)\n",
    "    \n",
    "    login_success_times = np.sum(recent_login_table['result']>0)\n",
    "    login_fail_times = np.sum(recent_login_table['result']<0)\n",
    "    \n",
    "    result_dict['trade_times'] = trade_times\n",
    "    result_dict['login_times'] = login_times\n",
    "    result_dict['login_success_times'] = login_success_times\n",
    "    result_dict['login_fail_times'] = login_fail_times\n",
    "    \n",
    "    if login_times ==0:\n",
    "        result_dict['trade_login_rate'] = -10\n",
    "    else:\n",
    "        result_dict['trade_login_rate'] = trade_times*1.0/login_times\n",
    "        \n",
    "    if login_times ==0:\n",
    "        result_dict['login_success_rate'] = -10\n",
    "    else:\n",
    "        result_dict['login_success_rate'] = login_success_times*1.0/login_times\n",
    "    \n",
    "    if login_success_times ==0:\n",
    "        result_dict['trade_login_success_rate'] = -10\n",
    "    else:\n",
    "        result_dict['trade_login_success_rate'] = trade_times*1.0/login_success_times\n",
    "    \n",
    "    if login_fail_times ==0:\n",
    "        result_dict['trade_login_fail_rate'] = -10\n",
    "    else:\n",
    "        result_dict['trade_login_fail_rate'] = trade_times*1.0/login_fail_times\n",
    "    \n",
    "    result_dict['multiple_fails'] = lower_than_zero_more_than_once(recent_login_table['result'])\n",
    "    result_dict['after_fail_mean'],result_dict['after_fail_max'],result_dict['after_fail_min'],result_dict['after_fail_med']\\\n",
    "    ,result_dict['after_fail_std'] = get_averge_fail_to_success_time(recent_login_table)\n",
    "    \n",
    "    timelong_series =  np.log(recent_login_table['timelong']+1).dropna()\n",
    "    if len(timelong_series) == 0:\n",
    "        result_dict['timelong_mean'] = -10\n",
    "        result_dict['timelong_max'] = -10\n",
    "        result_dict['timelong_min'] = -10\n",
    "        result_dict['timelong_med'] = -10\n",
    "        result_dict['timelong_std'] = -10\n",
    "    else:\n",
    "        result_dict['timelong_mean'] = np.mean(timelong_series)\n",
    "        result_dict['timelong_med'] = np.median(timelong_series)\n",
    "        result_dict['timelong_min'] = np.min(timelong_series)\n",
    "        result_dict['timelong_max'] = np.max(timelong_series)\n",
    "        if len(timelong_series) > 1:\n",
    "            result_dict['timelong_std'] =  np.std(timelong_series)\n",
    "        else:\n",
    "            result_dict['timelong_std'] = -10\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def lower_than_zero_more_than_once(sequence):\n",
    "    if len(sequence)>2:\n",
    "        sequence = list(sequence)\n",
    "        for i in range(len(sequence)-1):\n",
    "            if sequence[i] < 0:\n",
    "                if sequence[i+1]<0:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def get_averge_fail_to_success_time(recent_login_table):\n",
    "    login_table_process = recent_login_table[['result','time']].sort_values(by = 'time')\n",
    "    login_fail_times = np.sum(login_table_process['result'])\n",
    "    \n",
    "    if login_fail_times<1 or len(login_table_process)<2:\n",
    "        return (-10,-10,-10,-10,-10)\n",
    "    \n",
    "    time_delta_list = []\n",
    "    for i in range(len(login_table_process)-1):\n",
    "        if login_table_process.iloc[i].result < 0:\n",
    "            time_delta_list.append(login_table_process.iloc[i+1].time - login_table_process.iloc[i].time)\n",
    "            \n",
    "    time_delta_list = np.log(list(map(lambda x: x.total_seconds(),time_delta_list))+1)\n",
    "    \n",
    "    if len(time_delta_list) < 2:\n",
    "        std_return = -10\n",
    "    else:\n",
    "        std_return = np.std(time_delta_list)\n",
    "        \n",
    "    if len(time_delta_list) ==0:\n",
    "        return -10, -10, -10, -10,-10\n",
    "    \n",
    "    return np.mean(time_delta_list),np.max(time_delta_list),np.min(time_delta_list),np.median(time_delta_list),std_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nworks on spark\\nconf = spark.SparkConf().setAppName('jupyter_backend').setMaster('local[7]')        .set('spark.executor.memory','2g')        .set('spark.default.parallelism','112')\\nsc = spark.SparkContext(conf=conf)\\n\\n#packing the rdd for spark\\ntrade_tt_rdd_buffer = []\\nfor (idx,row) in trade_tt.iterrows():\\n    trade_tt_rdd_buffer.append(row)\\ntrade_tt_rdd = sc.parallelize(trade_tt_rdd_buffer)\\n\\nresult_rdd = trade_tt_rdd.map(lambda x : get_multiple_feature_dicts_wihtin_days_with_rowkey(x,login_tt,trade_tt))\\nresult_rdd_buffer = result_rdd.collect()\\n\\n#getting the new feature names\\nrecent_trade_example=find_related_recent_trades_within_days(trade_tt.loc[0],trade_tt,30)\\nrecent_login_example=find_related_recent_logins_within_days(trade_tt.loc[0],login_tt,30)\\n\\ndate_range_list = [1,3,7,15,30,360]\\nfeature_list = list(build_statistical_feature_dict(recent_login_example,recent_trade_example).keys())\\n\\n#unstacking the result_rdd_dict\\nresult_rdd_to_df_buffer = []\\nfor rowkey,result_dict in result_rdd_buffer:\\n    unit_dict= {}\\n    unit_dict['rowkey'] = rowkey\\n    \\n    for date_range in date_range_list:\\n        for feature in feature_list:\\n            unit_dict[feature+'_'+str(date_range)] = result_dict[date_range][feature]\\n    \\n    result_rdd_to_df_buffer.append(unit_dict)\\nresult_df = pd.DataFrame(result_rdd_to_df_buffer)\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trade_tt['stat_result_dicts'] = trade_tt.apply(lambda row : get_multiple_feature_dicts_wihtin_days(row,login_tt,trade_tt),axis = 1)\n",
    "#trade_tt.to_pickle(data_path+'trade_tt_stat_C_temp.pkl')\n",
    "\"\"\"\n",
    "works on spark\n",
    "conf = spark.SparkConf().setAppName('jupyter_backend').setMaster('local[7]')\\\n",
    "        .set('spark.executor.memory','2g')\\\n",
    "        .set('spark.default.parallelism','112')\n",
    "sc = spark.SparkContext(conf=conf)\n",
    "\n",
    "#packing the rdd for spark\n",
    "trade_tt_rdd_buffer = []\n",
    "for (idx,row) in trade_tt.iterrows():\n",
    "    trade_tt_rdd_buffer.append(row)\n",
    "trade_tt_rdd = sc.parallelize(trade_tt_rdd_buffer)\n",
    "\n",
    "result_rdd = trade_tt_rdd.map(lambda x : get_multiple_feature_dicts_wihtin_days_with_rowkey(x,login_tt,trade_tt))\n",
    "result_rdd_buffer = result_rdd.collect()\n",
    "\n",
    "#getting the new feature names\n",
    "recent_trade_example=find_related_recent_trades_within_days(trade_tt.loc[0],trade_tt,30)\n",
    "recent_login_example=find_related_recent_logins_within_days(trade_tt.loc[0],login_tt,30)\n",
    "\n",
    "date_range_list = [1,3,7,15,30,360]\n",
    "feature_list = list(build_statistical_feature_dict(recent_login_example,recent_trade_example).keys())\n",
    "\n",
    "#unstacking the result_rdd_dict\n",
    "result_rdd_to_df_buffer = []\n",
    "for rowkey,result_dict in result_rdd_buffer:\n",
    "    unit_dict= {}\n",
    "    unit_dict['rowkey'] = rowkey\n",
    "    \n",
    "    for date_range in date_range_list:\n",
    "        for feature in feature_list:\n",
    "            unit_dict[feature+'_'+str(date_range)] = result_dict[date_range][feature]\n",
    "    \n",
    "    result_rdd_to_df_buffer.append(unit_dict)\n",
    "result_df = pd.DataFrame(result_rdd_to_df_buffer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(150594, 119)\n"
     ]
    }
   ],
   "source": [
    "#load from spark generated features\n",
    "trade_tt_feature_c = pd.read_csv(data_path+'temp/feature_set_c.csv')\n",
    "print np.sum(trade_tt_feature_c['rowkey']!=trade_tt['rowkey'])\n",
    "del trade_tt_feature_c['rowkey']\n",
    "trade_tt = pd.concat([trade_tt,trade_tt_feature_c],axis = 1)\n",
    "print trade_tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "login_trade_hist_stats_feature_list = list(trade_tt.columns)\n",
    "login_trade_hist_stats_feature_list.remove('rowkey')\n",
    "login_trade_hist_stats_feature_list.remove('time')\n",
    "login_trade_hist_stats_feature_list.remove('id')\n",
    "login_trade_hist_stats_feature_list.remove('from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trade_tt = trade_tt.fillna(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in login_trade_hist_stats_feature_list:\n",
    "    pd.to_pickle(trade_tt[feature].values,feature_path+feature+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检测特征空值率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_emptyness = {}\n",
    "feature_emptyness_list = []\n",
    "data_size = trade_tt.shape[0]\n",
    "for feature in login_trade_hist_stats_feature_list:\n",
    "    feature_emptyness[feature] = np.sum(trade_tt[feature] == -10)*1.0/data_size\n",
    "    feature_emptyness_list.append((feature,feature_emptyness[feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('login_fail_times_1', 0.0),\n",
       " ('login_fail_times_15', 0.0),\n",
       " ('login_fail_times_3', 0.0),\n",
       " ('login_fail_times_30', 0.0),\n",
       " ('login_fail_times_360', 0.0),\n",
       " ('login_fail_times_7', 0.0),\n",
       " ('login_success_times_1', 0.0),\n",
       " ('login_success_times_15', 0.0),\n",
       " ('login_success_times_3', 0.0),\n",
       " ('login_success_times_30', 0.0),\n",
       " ('login_success_times_360', 0.0),\n",
       " ('login_success_times_7', 0.0),\n",
       " ('login_times_1', 0.0),\n",
       " ('login_times_15', 0.0),\n",
       " ('login_times_3', 0.0),\n",
       " ('login_times_30', 0.0),\n",
       " ('login_times_360', 0.0),\n",
       " ('login_times_7', 0.0),\n",
       " ('multiple_fails_1', 0.0),\n",
       " ('multiple_fails_15', 0.0),\n",
       " ('multiple_fails_3', 0.0),\n",
       " ('multiple_fails_30', 0.0),\n",
       " ('multiple_fails_360', 0.0),\n",
       " ('multiple_fails_7', 0.0),\n",
       " ('trade_times_1', 0.0),\n",
       " ('trade_times_15', 0.0),\n",
       " ('trade_times_3', 0.0),\n",
       " ('trade_times_30', 0.0),\n",
       " ('trade_times_360', 0.0),\n",
       " ('trade_times_7', 0.0),\n",
       " ('login_success_rate_360', 0.079050723893623581),\n",
       " ('trade_login_rate_360', 0.079050723893623581),\n",
       " ('timelong_max_360', 0.079082811376938081),\n",
       " ('timelong_mean_360', 0.079082811376938081),\n",
       " ('timelong_med_360', 0.079082811376938081),\n",
       " ('timelong_min_360', 0.079082811376938081),\n",
       " ('trade_login_success_rate_360', 0.08083478796591026),\n",
       " ('login_success_rate_30', 0.1804985111407742),\n",
       " ('trade_login_rate_30', 0.1804985111407742),\n",
       " ('timelong_max_30', 0.18053059862408871),\n",
       " ('timelong_mean_30', 0.18053059862408871),\n",
       " ('timelong_med_30', 0.18053059862408871),\n",
       " ('timelong_min_30', 0.18053059862408871),\n",
       " ('timelong_std_360', 0.18183335044665777),\n",
       " ('trade_login_success_rate_30', 0.18329012218913646),\n",
       " ('login_success_rate_15', 0.25379915802443781),\n",
       " ('trade_login_rate_15', 0.25379915802443781),\n",
       " ('timelong_max_15', 0.25383124550775232),\n",
       " ('timelong_mean_15', 0.25383124550775232),\n",
       " ('timelong_med_15', 0.25383124550775232),\n",
       " ('timelong_min_15', 0.25383124550775232),\n",
       " ('trade_login_success_rate_15', 0.25706566382585483),\n",
       " ('login_success_rate_7', 0.32961546359995891),\n",
       " ('trade_login_rate_7', 0.32961546359995891),\n",
       " ('timelong_max_7', 0.32964755108327343),\n",
       " ('timelong_mean_7', 0.32964755108327343),\n",
       " ('timelong_med_7', 0.32964755108327343),\n",
       " ('timelong_min_7', 0.32964755108327343),\n",
       " ('trade_login_success_rate_7', 0.3331450867645549),\n",
       " ('timelong_std_30', 0.33779777184515863),\n",
       " ('login_success_rate_3', 0.41608481363589689),\n",
       " ('trade_login_rate_3', 0.41608481363589689),\n",
       " ('timelong_max_3', 0.41611048362254854),\n",
       " ('timelong_mean_3', 0.41611048362254854),\n",
       " ('timelong_med_3', 0.41611048362254854),\n",
       " ('timelong_min_3', 0.41611048362254854),\n",
       " ('trade_login_success_rate_3', 0.41961443680049287),\n",
       " ('timelong_std_15', 0.43278955744943015),\n",
       " ('login_success_rate_1', 0.49722122394496354),\n",
       " ('trade_login_rate_1', 0.49722122394496354),\n",
       " ('timelong_max_1', 0.49724047643495223),\n",
       " ('timelong_mean_1', 0.49724047643495223),\n",
       " ('timelong_med_1', 0.49724047643495223),\n",
       " ('timelong_min_1', 0.49724047643495223),\n",
       " ('trade_login_success_rate_1', 0.50084710955950307),\n",
       " ('timelong_std_7', 0.53428226717322103),\n",
       " ('timelong_std_3', 0.64308450559605712),\n",
       " ('timelong_std_1', 0.76173760139644731),\n",
       " ('trade_login_fail_rate_360', 0.8636731183899784),\n",
       " ('after_fail_max_360', 0.87952433514734574),\n",
       " ('after_fail_mean_360', 0.87952433514734574),\n",
       " ('after_fail_med_360', 0.87952433514734574),\n",
       " ('after_fail_min_360', 0.87952433514734574),\n",
       " ('trade_login_fail_rate_30', 0.93695451278365338),\n",
       " ('after_fail_std_360', 0.95039275079576957),\n",
       " ('after_fail_max_30', 0.95295974946093032),\n",
       " ('after_fail_mean_30', 0.95295974946093032),\n",
       " ('after_fail_med_30', 0.95295974946093032),\n",
       " ('after_fail_min_30', 0.95295974946093032),\n",
       " ('trade_login_fail_rate_15', 0.95604014785912317),\n",
       " ('trade_login_fail_rate_7', 0.97109559503029064),\n",
       " ('after_fail_max_15', 0.9711020125269535),\n",
       " ('after_fail_mean_15', 0.9711020125269535),\n",
       " ('after_fail_med_15', 0.9711020125269535),\n",
       " ('after_fail_min_15', 0.9711020125269535),\n",
       " ('trade_login_fail_rate_3', 0.98157536708080917),\n",
       " ('after_fail_max_7', 0.98409744326932946),\n",
       " ('after_fail_mean_7', 0.98409744326932946),\n",
       " ('after_fail_med_7', 0.98409744326932946),\n",
       " ('after_fail_min_7', 0.98409744326932946),\n",
       " ('after_fail_std_30', 0.9865296745045693),\n",
       " ('trade_login_fail_rate_1', 0.98708799671424174),\n",
       " ('after_fail_max_3', 0.99202946914467605),\n",
       " ('after_fail_mean_3', 0.99202946914467605),\n",
       " ('after_fail_med_3', 0.99202946914467605),\n",
       " ('after_fail_min_3', 0.99202946914467605),\n",
       " ('after_fail_std_15', 0.99248511140774209),\n",
       " ('after_fail_max_1', 0.99587354964575414),\n",
       " ('after_fail_mean_1', 0.99587354964575414),\n",
       " ('after_fail_med_1', 0.99587354964575414),\n",
       " ('after_fail_min_1', 0.99587354964575414),\n",
       " ('after_fail_std_7', 0.99632919190882019),\n",
       " ('after_fail_std_3', 0.99810042098778107),\n",
       " ('after_fail_std_1', 0.99911438546051956)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(feature_emptyness_list,key = lambda x : x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
