{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Feature Generation(on Spark remote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'data_path', 'feature_dict', 'feature_path', 'model_features', 'result_path', 'single_module_validation_indice_set', 'trade_train_size', 'train_2_6_index']\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    pass\n",
    "config = pd.read_pickle('config.pkl')\n",
    "print(dir(config))\n",
    "\n",
    "data_path = '../../kaggleData/JD_logging/'\n",
    "feature_path = data_path+'features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "login_tt = pd.read_csv(data_path+'login_tt.csv')\n",
    "trade_tt = pd.read_csv(data_path+'trade_tt.csv')\n",
    "\n",
    "login_tt['time'] = login_tt['time'].apply(lambda x : datetime.datetime.strptime(x , '%Y-%m-%d %H:%M:%S'))\n",
    "trade_tt['time'] = trade_tt['time'].apply(lambda x : datetime.datetime.strptime(x , '%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_related_logins_before(row,login_table,*args,**kw):\n",
    "    related_logins = login_table[login_table['id'] == row.id]\n",
    "    related_logins_before = related_logins[related_logins['time']<row.time]\n",
    "    return related_logins_before    \n",
    "\n",
    "def find_related_recent_logins_within_days(row,login_table,days,*args,**kw):    \n",
    "    recent_logins = find_related_logins_before(row,login_table)\n",
    "    if len(recent_logins)>0:\n",
    "        recent_logins['from_now'] =  row.time - recent_logins['time']\n",
    "        return recent_logins[recent_logins['from_now']<datetime.timedelta(days = days)]\n",
    "    else:\n",
    "        recent_logins['from_now'] = np.nan\n",
    "        return recent_logins[recent_logins['from_now']<datetime.timedelta(days = days)]\n",
    "        \n",
    "\n",
    "def find_related_trades_before(row,trade_table,*args,**kw):\n",
    "    related_trades = trade_table[trade_table['id'] == row.id]\n",
    "    related_trades_before = related_trades[related_trades['time']<row.time]\n",
    "    return related_trades_before    \n",
    "\n",
    "def find_related_recent_trades_within_days(row,trade_table,days,*args,**kw):    \n",
    "    recent_trades = find_related_trades_before(row,trade_table)\n",
    "    if len(recent_trades)>0:\n",
    "        recent_trades['from_now'] =  row.time - recent_trades['time']\n",
    "        return recent_trades[recent_trades['from_now']<datetime.timedelta(days = days)]  \n",
    "    else:\n",
    "        recent_trades['from_now'] = np.nan\n",
    "        return recent_trades[recent_trades['from_now']<datetime.timedelta(days = days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_multiple_feature_dicts_wihtin_days(row,login_table,trade_table):\n",
    "    date_range = [360,30,15,7,3,1]\n",
    "    result_dict = {}\n",
    "    for days in date_range:\n",
    "        if days ==360:\n",
    "            recent_trade_table = find_related_recent_trades_within_days(row,trade_table,days)\n",
    "            recent_login_table = find_related_recent_logins_within_days(row,login_table,days)\n",
    "        else:\n",
    "            recent_trade_table = find_related_recent_trades_within_days(row,recent_trade_table,days)\n",
    "            recent_login_table = find_related_recent_logins_within_days(row,recent_login_table,days)\n",
    "        \n",
    "        result_dict[days] = build_statistical_feature_dict(recent_login_table,recent_trade_table)    \n",
    "    return result_dict\n",
    "\n",
    "def get_multiple_feature_dicts_wihtin_days_with_rowkey(row_index_tuple,login_table,trade_table):\n",
    "    ori_idx,row = row_index_tuple\n",
    "    return (ori_idx,row.rowkey,get_multiple_feature_dicts_wihtin_days(row,login_table,trade_table))\n",
    "\n",
    "def build_statistical_feature_dict(recent_login_table,recent_trade_table,*args,**kw):\n",
    "    \"\"\"\n",
    "    ID交易次数\n",
    "    #最近的前一次交易时间 - 在顶层使用\n",
    "    ID登录次数\n",
    "    交易/登录次数比\n",
    "    ID登录成功次数（大于零的项）\n",
    "    ID登录失败次数（小于零的项）\n",
    "    ID登录成功比率\n",
    "    交易/成功登录次数比，交易/失败次数比\n",
    "    是否有连续login失败\n",
    "    login失败到下一次尝试的平均时间、最大时间、最小时间、时间中位数、方差\n",
    "    timelong平均值，最大值，最小值，中位数，方差\n",
    "    timelong方差（仅一个时为0或N/A）\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    \n",
    "    trade_times = len(recent_trade_table)\n",
    "    login_times = len(recent_login_table)\n",
    "    \n",
    "    login_success_times = np.sum(recent_login_table['result']>0)\n",
    "    login_fail_times = np.sum(recent_login_table['result']<0)\n",
    "    \n",
    "    result_dict['trade_times'] = trade_times\n",
    "    result_dict['login_times'] = login_times\n",
    "    result_dict['login_success_times'] = login_success_times\n",
    "    result_dict['login_fail_times'] = login_fail_times\n",
    "    \n",
    "    if login_times ==0:\n",
    "        if trade_times >0:\n",
    "            result_dict['trade_login_rate'] = 100\n",
    "        else:\n",
    "             result_dict['trade_login_rate'] = 0\n",
    "    else:\n",
    "        result_dict['trade_login_rate'] = trade_times*1.0/login_times\n",
    "        \n",
    "    if login_times ==0:\n",
    "        result_dict['login_success_rate'] = 0\n",
    "    else:\n",
    "        result_dict['login_success_rate'] = login_success_times*1.0/login_times\n",
    "    \n",
    "    if login_success_times ==0:\n",
    "        if trade_times >0:\n",
    "            result_dict['trade_login_success_rate'] = 100\n",
    "        else:\n",
    "             result_dict['trade_login_success_rate'] = 0\n",
    "    else:\n",
    "        result_dict['trade_login_success_rate'] = trade_times*1.0/login_success_times\n",
    "    \n",
    "    result_dict['multiple_fails'] = lower_than_zero_more_than_once(recent_login_table['result'])\n",
    "    result_dict['after_fail_mean'],result_dict['after_fail_max'],result_dict['after_fail_min'],result_dict['after_fail_med']\\\n",
    "    ,result_dict['after_fail_std'] = get_averge_fail_to_success_time(recent_login_table)\n",
    "    \n",
    "    timelong_series =  np.log(recent_login_table['timelong']+1).dropna()\n",
    "    if len(timelong_series) == 0:\n",
    "        result_dict['timelong_mean'] = -10\n",
    "        result_dict['timelong_max'] = -10\n",
    "        result_dict['timelong_min'] = -10\n",
    "        result_dict['timelong_med'] = -10\n",
    "        result_dict['timelong_std'] = -10\n",
    "    else:\n",
    "        result_dict['timelong_mean'] = np.mean(timelong_series)\n",
    "        result_dict['timelong_med'] = np.median(timelong_series)\n",
    "        result_dict['timelong_min'] = np.min(timelong_series)\n",
    "        result_dict['timelong_max'] = np.max(timelong_series)\n",
    "        if len(timelong_series) > 1:\n",
    "            result_dict['timelong_std'] =  np.std(timelong_series)\n",
    "        else:\n",
    "            result_dict['timelong_std'] = -10\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def lower_than_zero_more_than_once(sequence):\n",
    "    if len(sequence)>2:\n",
    "        sequence = list(sequence)\n",
    "        for i in range(len(sequence)-1):\n",
    "            if sequence[i] < 0:\n",
    "                if sequence[i+1]<0:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def get_averge_fail_to_success_time(recent_login_table):\n",
    "    login_table_process = recent_login_table[['result','time']].sort_values(by = 'time')\n",
    "    login_fail_times = np.sum(login_table_process['result'])\n",
    "    \n",
    "    if login_fail_times<1 or len(login_table_process)<2:\n",
    "        return (-10,-10,-10,-10,-10)\n",
    "    \n",
    "    time_delta_list = []\n",
    "    \n",
    "    for i in range(len(login_table_process)-1):\n",
    "        if login_table_process.iloc[i].result < 0:\n",
    "            time_delta_list.append(login_table_process.iloc[i+1].time - login_table_process.iloc[i].time)\n",
    "\n",
    "    if len(time_delta_list) ==0:\n",
    "        return -10, -10, -10, -10,-10\n",
    "    #python 2.7 map usage\n",
    "    #time_delta_list = np.log(map(lambda x: x.total_seconds(),time_delta_list))\n",
    "    #python 3.4 map usage\n",
    "    time_delta_list = np.log(list(map(lambda x: x.total_seconds()+1,time_delta_list)))\n",
    "    \n",
    "    if len(time_delta_list) < 2:\n",
    "        std_return = -10\n",
    "    else:\n",
    "        std_return = np.std(time_delta_list)\n",
    "\n",
    "    return np.mean(time_delta_list),np.max(time_delta_list),np.min(time_delta_list),np.median(time_delta_list),std_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = spark.SparkConf().setAppName('jupyter_backend').setMaster('local[15]')\\\n",
    "        .set('spark.executor.memory','2g')\\\n",
    "        .set('spark.default.parallelism','112')\n",
    "sc = spark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"packing the rdd for spark\"\"\"\n",
    "trade_tt_rdd_buffer = []\n",
    "for (idx,row) in trade_tt.iterrows():\n",
    "    trade_tt_rdd_buffer.append((idx,row))\n",
    "trade_tt_rdd = sc.parallelize(trade_tt_rdd_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the prototype in pandas is :\n",
    "trade_tt['stat_result_dicts'] = trade_tt.apply(lambda row : get_multiple_feature_dicts_wihtin_days(row,login_tt,trade_tt),axis = 1)\n",
    "trade_tt.to_pickle(data_path+'trade_tt_stat_C_temp.pkl')\n",
    "\n",
    "rewriting in spark and get the result\n",
    "\n",
    "as only working on single node, no need to broadcast\n",
    "\n",
    "using the original index in the tuple to identify the location\n",
    "\"\"\"\n",
    "result_rdd = trade_tt_rdd.map(lambda x : get_multiple_feature_dicts_wihtin_days_with_rowkey(x,login_tt,trade_tt))\n",
    "result_rdd_buffer = result_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "getting the new feature names\n",
    "\"\"\"\n",
    "recent_trade_example=find_related_recent_trades_within_days(trade_tt.loc[0],trade_tt,30)\n",
    "recent_login_example=find_related_recent_logins_within_days(trade_tt.loc[0],login_tt,30)\n",
    "\n",
    "date_range_list = [1,3,7,15,30,360]\n",
    "feature_list = list(build_statistical_feature_dict(recent_login_example,recent_trade_example).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"unstacking the result_rdd_dict\"\"\"\n",
    "result_rdd_to_df_buffer = []\n",
    "for ori_idx,rowkey,result_dict in result_rdd_buffer:\n",
    "    unit_dict= {}\n",
    "    unit_dict['rowkey'] = rowkey\n",
    "    unit_dict['ori_idx'] = ori_idx\n",
    "    \n",
    "    for date_range in date_range_list:\n",
    "        for feature in feature_list:\n",
    "            unit_dict[feature+'_'+str(date_range)] = result_dict[date_range][feature]\n",
    "    \n",
    "    result_rdd_to_df_buffer.append(unit_dict)\n",
    "result_df = pd.DataFrame(result_rdd_to_df_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in result_df.columns:\n",
    "    if np.sum(result_df[feature]==-np.inf)>0:\n",
    "        print(feature,\" with -inf\")\n",
    "    if np.sum(np.isnan(result_df[feature]))>0:\n",
    "        print(feature,\"has nan\")\n",
    "    if np.sum(result_df[feature]==np.inf)>0:\n",
    "        print(feature,\" with inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df.to_csv(feature_path+'feature_set_c.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(150594, 119)\n"
     ]
    }
   ],
   "source": [
    "#load from spark generated features\n",
    "trade_tt_feature_c = pd.read_csv(data_path+'temp/feature_set_c.csv')\n",
    "print(np.sum(trade_tt_feature_c['rowkey']!=trade_tt['rowkey']))\n",
    "del trade_tt_feature_c['rowkey']\n",
    "trade_tt = pd.concat([trade_tt,trade_tt_feature_c],axis = 1)\n",
    "print(trade_tt.shape)\n",
    "\n",
    "login_trade_hist_stats_feature_list = list(trade_tt.columns)\n",
    "login_trade_hist_stats_feature_list.remove('rowkey')\n",
    "login_trade_hist_stats_feature_list.remove('time')\n",
    "login_trade_hist_stats_feature_list.remove('id')\n",
    "login_trade_hist_stats_feature_list.remove('from')\n",
    "\n",
    "trade_tt = trade_tt.fillna(-10)\n",
    "\n",
    "for feature in login_trade_hist_stats_feature_list:\n",
    "    pd.to_pickle(trade_tt[feature].values,feature_path+feature+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_emptyness = {}\n",
    "feature_emptyness_list = []\n",
    "data_size = trade_tt.shape[0]\n",
    "for feature in login_trade_hist_stats_feature_list:\n",
    "    feature_emptyness[feature] = np.sum(trade_tt[feature] == -10)*1.0/data_size\n",
    "    feature_emptyness_list.append((feature,feature_emptyness[feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('login_fail_times_1', 0.0),\n",
       " ('login_fail_times_15', 0.0),\n",
       " ('login_fail_times_3', 0.0),\n",
       " ('login_fail_times_30', 0.0),\n",
       " ('login_fail_times_360', 0.0),\n",
       " ('login_fail_times_7', 0.0),\n",
       " ('login_success_times_1', 0.0),\n",
       " ('login_success_times_15', 0.0),\n",
       " ('login_success_times_3', 0.0),\n",
       " ('login_success_times_30', 0.0),\n",
       " ('login_success_times_360', 0.0),\n",
       " ('login_success_times_7', 0.0),\n",
       " ('login_times_1', 0.0),\n",
       " ('login_times_15', 0.0),\n",
       " ('login_times_3', 0.0),\n",
       " ('login_times_30', 0.0),\n",
       " ('login_times_360', 0.0),\n",
       " ('login_times_7', 0.0),\n",
       " ('multiple_fails_1', 0.0),\n",
       " ('multiple_fails_15', 0.0),\n",
       " ('multiple_fails_3', 0.0),\n",
       " ('multiple_fails_30', 0.0),\n",
       " ('multiple_fails_360', 0.0),\n",
       " ('multiple_fails_7', 0.0),\n",
       " ('ori_idx', 0.0),\n",
       " ('trade_times_1', 0.0),\n",
       " ('trade_times_15', 0.0),\n",
       " ('trade_times_3', 0.0),\n",
       " ('trade_times_30', 0.0),\n",
       " ('trade_times_360', 0.0),\n",
       " ('trade_times_7', 0.0),\n",
       " ('login_success_rate_360', 0.077307196833871203),\n",
       " ('trade_login_rate_360', 0.077307196833871203),\n",
       " ('timelong_max_360', 0.077333758317064419),\n",
       " ('timelong_mean_360', 0.077333758317064419),\n",
       " ('timelong_med_360', 0.077333758317064419),\n",
       " ('timelong_min_360', 0.077333758317064419),\n",
       " ('trade_login_success_rate_360', 0.079073535466220438),\n",
       " ('login_success_rate_30', 0.17934313452063164),\n",
       " ('trade_login_rate_30', 0.17934313452063164),\n",
       " ('timelong_max_30', 0.17936969600382485),\n",
       " ('timelong_mean_30', 0.17936969600382485),\n",
       " ('timelong_med_30', 0.17936969600382485),\n",
       " ('timelong_min_30', 0.17936969600382485),\n",
       " ('timelong_std_360', 0.17952906490298418),\n",
       " ('trade_login_success_rate_30', 0.18213209025591989),\n",
       " ('login_success_rate_15', 0.25338326892173657),\n",
       " ('trade_login_rate_15', 0.25338326892173657),\n",
       " ('timelong_max_15', 0.25340983040492981),\n",
       " ('timelong_mean_15', 0.25340983040492981),\n",
       " ('timelong_med_15', 0.25340983040492981),\n",
       " ('timelong_min_15', 0.25340983040492981),\n",
       " ('trade_login_success_rate_15', 0.25667025246689773),\n",
       " ('login_success_rate_7', 0.32930262825876194),\n",
       " ('trade_login_rate_7', 0.32930262825876194),\n",
       " ('timelong_max_7', 0.32932918974195519),\n",
       " ('timelong_mean_7', 0.32932918974195519),\n",
       " ('timelong_med_7', 0.32932918974195519),\n",
       " ('timelong_min_7', 0.32932918974195519),\n",
       " ('trade_login_success_rate_7', 0.33283530552346041),\n",
       " ('timelong_std_30', 0.33636798278815888),\n",
       " ('login_success_rate_3', 0.41550792196236236),\n",
       " ('trade_login_rate_3', 0.41550792196236236),\n",
       " ('timelong_max_3', 0.41552784307475732),\n",
       " ('timelong_mean_3', 0.41552784307475732),\n",
       " ('timelong_med_3', 0.41552784307475732),\n",
       " ('timelong_min_3', 0.41552784307475732),\n",
       " ('trade_login_success_rate_3', 0.41902731848546421),\n",
       " ('timelong_std_15', 0.43188971672178172),\n",
       " ('login_success_rate_1', 0.49669309534244394),\n",
       " ('trade_login_rate_1', 0.49669309534244394),\n",
       " ('timelong_max_1', 0.49670637608404056),\n",
       " ('timelong_mean_1', 0.49670637608404056),\n",
       " ('timelong_med_1', 0.49670637608404056),\n",
       " ('timelong_min_1', 0.49670637608404056),\n",
       " ('trade_login_success_rate_1', 0.50030545705672202),\n",
       " ('timelong_std_7', 0.53366667994740824),\n",
       " ('timelong_std_3', 0.64250899770243175),\n",
       " ('timelong_std_1', 0.76145795981247588),\n",
       " ('trade_login_fail_rate_360', 0.8633743708248669),\n",
       " ('after_fail_max_360', 0.8792382166620184),\n",
       " ('after_fail_mean_360', 0.8792382166620184),\n",
       " ('after_fail_med_360', 0.8792382166620184),\n",
       " ('after_fail_min_360', 0.8792382166620184),\n",
       " ('trade_login_fail_rate_30', 0.9365114147974023),\n",
       " ('after_fail_std_360', 0.95043627236144868),\n",
       " ('after_fail_max_30', 0.95253462953371315),\n",
       " ('after_fail_mean_30', 0.95253462953371315),\n",
       " ('after_fail_med_30', 0.95253462953371315),\n",
       " ('after_fail_min_30', 0.95253462953371315),\n",
       " ('trade_login_fail_rate_15', 0.95587473604526074),\n",
       " ('after_fail_max_15', 0.97091517590342247),\n",
       " ('after_fail_mean_15', 0.97091517590342247),\n",
       " ('after_fail_med_15', 0.97091517590342247),\n",
       " ('after_fail_min_15', 0.97091517590342247),\n",
       " ('trade_login_fail_rate_7', 0.97109446591497672),\n",
       " ('trade_login_fail_rate_3', 0.98162609400108902),\n",
       " ('after_fail_max_7', 0.98403654860087386),\n",
       " ('after_fail_mean_7', 0.98403654860087386),\n",
       " ('after_fail_med_7', 0.98403654860087386),\n",
       " ('after_fail_min_7', 0.98403654860087386),\n",
       " ('after_fail_std_30', 0.98641380134666723),\n",
       " ('trade_login_fail_rate_1', 0.98717080361767406),\n",
       " ('after_fail_max_3', 0.99199835318804197),\n",
       " ('after_fail_mean_3', 0.99199835318804197),\n",
       " ('after_fail_med_3', 0.99199835318804197),\n",
       " ('after_fail_min_3', 0.99199835318804197),\n",
       " ('after_fail_std_15', 0.99241005617753697),\n",
       " ('after_fail_max_1', 0.99587632973425233),\n",
       " ('after_fail_mean_1', 0.99587632973425233),\n",
       " ('after_fail_med_1', 0.99587632973425233),\n",
       " ('after_fail_min_1', 0.99587632973425233),\n",
       " ('after_fail_std_7', 0.99630795383614223),\n",
       " ('after_fail_std_3', 0.99811413469328125),\n",
       " ('after_fail_std_1', 0.99913011142542196)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(feature_emptyness_list,key = lambda x : x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
