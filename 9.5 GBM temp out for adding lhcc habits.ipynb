{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt \n",
    "import datetime\n",
    "from sklearn.metrics import fbeta_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'data_path', 'feature_dict', 'feature_path', 'model_features', 'result_path', 'single_module_validation_indice_set', 'trade_train_size', 'train_2_6_index']\n",
      "dict_keys(['trade_and_recent_login_comparing', 'recent_login_detail', 'trade_detail_feature', 'login_trade_hist_stats', 'llc_user_habbit', 'hcc_user_habbit'])\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    pass\n",
    "config = pd.read_pickle('config.pkl')\n",
    "data_path = config.data_path\n",
    "feature_path = config.feature_path\n",
    "print(dir(config))\n",
    "print(config.feature_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model define\n",
    "\n",
    "def f_beta_01(preds, train_data):\n",
    "    labels  = train_data.get_label()\n",
    "    return 'fbeta_score_01',fbeta_score(labels, preds > 0.8,0.1),True\n",
    "\n",
    "    \n",
    "#for binary\n",
    "def runLGBM(train_X, train_y, test_X, test_y=None, feature_names=None,\n",
    "           seed_val=0, num_rounds=10000,watch_dict = None,max_bin=50000,\n",
    "           num_leaves=16,early_stop=64,verbose=True,eta=0.1,\n",
    "           bagging_fraction = 0.75 , feature_fraction = 0.75,feval = None,metric = 'binary_logloss',\n",
    "           train_sample_weight = None):\n",
    "    \n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': num_leaves,\n",
    "        'learning_rate': eta,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': verbose,\n",
    "        'is_unbalance':False\n",
    "    }\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    #plst = list(param.items())\n",
    "    lgbtrain = lgb.Dataset(train_X, label=train_y,max_bin=max_bin,feature_name=feature_names,weight =train_sample_weight)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgbtest = lgb.Dataset(test_X, label=test_y,max_bin=max_bin,feature_name=feature_names)\n",
    "        watchlist = [lgbtrain,lgbtest]\n",
    "        watchlist_name=['train','test']\n",
    "        model = lgb.train(params, lgbtrain, num_rounds, watchlist,watchlist_name, early_stopping_rounds=early_stop,\\\n",
    "                         evals_result = watch_dict,verbose_eval=verbose,feval = feval)\n",
    "    else:\n",
    "        #lgbtest = lgb.Dataset(test_X,feature_name=feature_names)\n",
    "        model = lgb.train(params, lgbtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features =   (config.feature_dict['trade_detail_feature']+\n",
    "              config.feature_dict['recent_login_detail']+\n",
    "              config.feature_dict['trade_and_recent_login_comparing']+\n",
    "              config.feature_dict['login_trade_hist_stats']+\n",
    "              config.feature_dict['llc_user_habbit']+\n",
    "             config.feature_dict['hcc_user_habbit'])\n",
    "feature_sequence_list = []\n",
    "for feature in features:\n",
    "    feature_sequence_list.append(pd.read_pickle(feature_path+feature+'.pkl').reshape(-1,1))\n",
    "    \n",
    "trade_tt_mat = np.hstack(feature_sequence_list)\n",
    "#trade_tt_mat[trade_tt_mat==-10]=np.nan\n",
    "\n",
    "validation_tuple_list = config.single_module_validation_indice_set\n",
    "train_labels = pd.read_pickle(data_path+'trade_train_label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's auc: 0.957468\ttrain's fbeta_score_01: 0.952373\ttest's auc: 0.940344\ttest's fbeta_score_01: 0.769314\n",
      "[200]\ttrain's auc: 0.98396\ttrain's fbeta_score_01: 0.978274\ttest's auc: 0.967108\ttest's fbeta_score_01: 0.83637\n",
      "[300]\ttrain's auc: 0.993427\ttrain's fbeta_score_01: 0.977495\ttest's auc: 0.970701\ttest's fbeta_score_01: 0.894003\n",
      "[400]\ttrain's auc: 0.998044\ttrain's fbeta_score_01: 0.990003\ttest's auc: 0.971128\ttest's fbeta_score_01: 0.89672\n",
      "[500]\ttrain's auc: 0.999273\ttrain's fbeta_score_01: 0.992133\ttest's auc: 0.971324\ttest's fbeta_score_01: 0.881134\n",
      "[600]\ttrain's auc: 0.999764\ttrain's fbeta_score_01: 0.995836\ttest's auc: 0.969427\ttest's fbeta_score_01: 0.867618\n",
      "f_beta score for the turn 1 is 0.867617732781\n",
      "[100]\ttrain's auc: 0.970805\ttrain's fbeta_score_01: 0.958669\ttest's auc: 0.870826\ttest's fbeta_score_01: 0\n",
      "[200]\ttrain's auc: 0.988744\ttrain's fbeta_score_01: 0.970419\ttest's auc: 0.907683\ttest's fbeta_score_01: 0\n",
      "[300]\ttrain's auc: 0.996786\ttrain's fbeta_score_01: 0.981167\ttest's auc: 0.920903\ttest's fbeta_score_01: 0.748148\n",
      "[400]\ttrain's auc: 0.999025\ttrain's fbeta_score_01: 0.990155\ttest's auc: 0.927863\ttest's fbeta_score_01: 0.781215\n",
      "[500]\ttrain's auc: 0.999583\ttrain's fbeta_score_01: 0.993551\ttest's auc: 0.929458\ttest's fbeta_score_01: 0.781215\n",
      "[600]\ttrain's auc: 0.999846\ttrain's fbeta_score_01: 0.996746\ttest's auc: 0.92925\ttest's fbeta_score_01: 0.696552\n",
      "f_beta score for the turn 2 is 0.696551724138\n",
      "[100]\ttrain's auc: 0.967107\ttrain's fbeta_score_01: 0.960506\ttest's auc: 0.921907\ttest's fbeta_score_01: 0.893805\n",
      "[200]\ttrain's auc: 0.987601\ttrain's fbeta_score_01: 0.975465\ttest's auc: 0.949357\ttest's fbeta_score_01: 0.889868\n",
      "[300]\ttrain's auc: 0.996013\ttrain's fbeta_score_01: 0.982831\ttest's auc: 0.959041\ttest's fbeta_score_01: 0.846409\n",
      "[400]\ttrain's auc: 0.998826\ttrain's fbeta_score_01: 0.989233\ttest's auc: 0.96241\ttest's fbeta_score_01: 0.857432\n",
      "[500]\ttrain's auc: 0.999576\ttrain's fbeta_score_01: 0.993259\ttest's auc: 0.96263\ttest's fbeta_score_01: 0.868844\n",
      "[600]\ttrain's auc: 0.999857\ttrain's fbeta_score_01: 0.996567\ttest's auc: 0.961893\ttest's fbeta_score_01: 0.858146\n",
      "f_beta score for the turn 3 is 0.858145548578\n",
      "[100]\ttrain's auc: 0.96266\ttrain's fbeta_score_01: 0.959073\ttest's auc: 0.892023\ttest's fbeta_score_01: 0\n",
      "[200]\ttrain's auc: 0.984705\ttrain's fbeta_score_01: 0.973853\ttest's auc: 0.934062\ttest's fbeta_score_01: 0.647436\n",
      "[300]\ttrain's auc: 0.996809\ttrain's fbeta_score_01: 0.98679\ttest's auc: 0.948364\ttest's fbeta_score_01: 0.825163\n",
      "[400]\ttrain's auc: 0.998929\ttrain's fbeta_score_01: 0.992072\ttest's auc: 0.950898\ttest's fbeta_score_01: 0.746305\n",
      "[500]\ttrain's auc: 0.999599\ttrain's fbeta_score_01: 0.995132\ttest's auc: 0.952388\ttest's fbeta_score_01: 0.775219\n",
      "[600]\ttrain's auc: 0.999867\ttrain's fbeta_score_01: 0.996465\ttest's auc: 0.952602\ttest's fbeta_score_01: 0.798419\n",
      "f_beta score for the turn 4 is 0.798418972332\n",
      "[100]\ttrain's auc: 0.949741\ttrain's fbeta_score_01: 0.960401\ttest's auc: 0.92015\ttest's fbeta_score_01: 0.687075\n",
      "[200]\ttrain's auc: 0.983218\ttrain's fbeta_score_01: 0.980545\ttest's auc: 0.953259\ttest's fbeta_score_01: 0.854867\n",
      "[300]\ttrain's auc: 0.996043\ttrain's fbeta_score_01: 0.978669\ttest's auc: 0.963048\ttest's fbeta_score_01: 0.847997\n",
      "[400]\ttrain's auc: 0.998882\ttrain's fbeta_score_01: 0.989167\ttest's auc: 0.96306\ttest's fbeta_score_01: 0.83419\n",
      "[500]\ttrain's auc: 0.999567\ttrain's fbeta_score_01: 0.993712\ttest's auc: 0.962638\ttest's fbeta_score_01: 0.842079\n",
      "[600]\ttrain's auc: 0.99985\ttrain's fbeta_score_01: 0.9955\ttest's auc: 0.961569\ttest's fbeta_score_01: 0.831407\n",
      "f_beta score for the turn 5 is 0.831407313598\n",
      "[100]\ttrain's auc: 0.965507\ttrain's fbeta_score_01: 0.944718\ttest's auc: 0.884421\ttest's fbeta_score_01: 0\n",
      "[200]\ttrain's auc: 0.985812\ttrain's fbeta_score_01: 0.96996\ttest's auc: 0.930808\ttest's fbeta_score_01: 0.5\n",
      "[300]\ttrain's auc: 0.996407\ttrain's fbeta_score_01: 0.973427\ttest's auc: 0.942827\ttest's fbeta_score_01: 0.501656\n",
      "[400]\ttrain's auc: 0.998588\ttrain's fbeta_score_01: 0.984187\ttest's auc: 0.94771\ttest's fbeta_score_01: 0.628109\n",
      "[500]\ttrain's auc: 0.999388\ttrain's fbeta_score_01: 0.992733\ttest's auc: 0.950055\ttest's fbeta_score_01: 0.628109\n",
      "[600]\ttrain's auc: 0.999724\ttrain's fbeta_score_01: 0.99578\ttest's auc: 0.95043\ttest's fbeta_score_01: 0.670354\n",
      "f_beta score for the turn 6 is 0.670353982301\n",
      "The mean of the cv_scores is: 0.787082545621\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "cv_result = []\n",
    "models = []\n",
    "i = 0\n",
    "\n",
    "for train_indice,val_indice in validation_tuple_list:\n",
    "    #print trade_train_val.iloc[train_indice]['month'].unique(),trade_train_val.iloc[val_indice]['month'].unique()\n",
    "    #print trade_train_val.iloc[train_indice].shape,trade_train_val.iloc[val_indice].shape\n",
    "    result_dict = {}\n",
    "    \n",
    "    #filter the features\n",
    "    dev_X, val_X = trade_tt_mat[train_indice], trade_tt_mat[val_indice]\n",
    "    dev_y, val_y = train_labels.iloc[train_indice].values, train_labels.iloc[val_indice].values\n",
    "\n",
    "    \n",
    "    preds, model = runLGBM(dev_X, dev_y, val_X, val_y,feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=600,watch_dict=result_dict,feval = f_beta_01)\n",
    "\n",
    "    #result_f_beta = f_beta_01(val_y.values, preds>0.5)\n",
    "    result_f_beta  = fbeta_score( val_y,preds > 0.8, 0.1)\n",
    "    \n",
    "    cv_scores.append(result_f_beta)\n",
    "    cv_result.append(result_dict)\n",
    "    models.append(model)\n",
    "    i+=1\n",
    "    print('f_beta score for the turn '+str(i)+' is '+str(result_f_beta))\n",
    "\n",
    "print('The mean of the cv_scores is:',np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n",
      "357\n"
     ]
    }
   ],
   "source": [
    "#finding the best iteration\n",
    "pd_list = []\n",
    "for dic in cv_result:\n",
    "    pd_list.append(pd.DataFrame(dic['test']))\n",
    "for i in range(len(pd_list)):\n",
    "    pd_list[i].columns = pd_list[i].columns+'_'+str(i)\n",
    "validation_result = pd.concat(pd_list,axis = 1)\n",
    "validation_result['auc_avg'] = validation_result.apply(lambda x : np.mean([x.auc_0,x.auc_1,x.auc_2,x.auc_3,x.auc_4]),axis = 1)\n",
    "validation_result['fbeta_avg'] = validation_result.apply(lambda x : np.mean([x.fbeta_score_01_0,x.fbeta_score_01_1,\n",
    "                                                                     x.fbeta_score_01_2,x.fbeta_score_01_3,\n",
    "                                                                     x.fbeta_score_01_4]),axis=1)\n",
    "print(validation_result['auc_avg'].idxmax())\n",
    "print(validation_result['fbeta_avg'].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "train_X = trade_tt_mat[config.train_2_6_index]\n",
    "test_X = trade_tt_mat[config.trade_train_size:]\n",
    "train_y = train_labels[config.train_2_6_index]\n",
    "\n",
    "preds, _ = runLGBM(train_X, train_y, test_X, feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=357,watch_dict=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17875, 2)\n"
     ]
    }
   ],
   "source": [
    "result_path = '../../kaggleData/JD_logging/result/'\n",
    "test_rowkey = pd.read_pickle(data_path+'trade_test_rowkey.pkl')\n",
    "pred_label = pd.Series(preds > 0.5)\n",
    "result_set = pd.DataFrame(test_rowkey)\n",
    "result_set['is_risk'] = pred_label.astype(int)\n",
    "\n",
    "print(result_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(pred_label,result_path+'adding_type_f_357.pkl')\n",
    "result_set.to_csv(result_path+'adding_type_f_357.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
