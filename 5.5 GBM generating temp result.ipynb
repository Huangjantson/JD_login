{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt \n",
    "import datetime\n",
    "from sklearn.metrics import fbeta_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'data_path', 'feature_dict', 'feature_path', 'model_features', 'result_path', 'single_module_validation_indice_set', 'trade_train_size', 'train_2_6_index']\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    pass\n",
    "config = pd.read_pickle('config.pkl')\n",
    "data_path = config.data_path\n",
    "feature_path = config.feature_path\n",
    "print(dir(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model define\n",
    "\n",
    "def f_beta_01(preds, train_data):\n",
    "    labels  = train_data.get_label()\n",
    "    return 'fbeta_score_01',fbeta_score(labels, preds > 0.5,0.1),True\n",
    "\n",
    "    \n",
    "#for binary\n",
    "def runLGBM(train_X, train_y, test_X, test_y=None, feature_names=None,\n",
    "           seed_val=0, num_rounds=10000,watch_dict = None,max_bin=50000,\n",
    "           num_leaves=16,early_stop=64,verbose=True,eta=0.1,\n",
    "           bagging_fraction = 0.75 , feature_fraction = 0.75,feval = None,metric = 'binary_logloss',\n",
    "           train_sample_weight = None):\n",
    "    \n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': num_leaves,\n",
    "        'learning_rate': eta,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': verbose,\n",
    "        'is_unbalance':False\n",
    "    }\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    #plst = list(param.items())\n",
    "    lgbtrain = lgb.Dataset(train_X, label=train_y,max_bin=max_bin,feature_name=feature_names,weight =train_sample_weight)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgbtest = lgb.Dataset(test_X, label=test_y,max_bin=max_bin,feature_name=feature_names)\n",
    "        watchlist = [lgbtrain,lgbtest]\n",
    "        watchlist_name=['train','test']\n",
    "        model = lgb.train(params, lgbtrain, num_rounds, watchlist,watchlist_name, early_stopping_rounds=early_stop,\\\n",
    "                         evals_result = watch_dict,verbose_eval=verbose,feval = feval)\n",
    "    else:\n",
    "        #lgbtest = lgb.Dataset(test_X,feature_name=feature_names)\n",
    "        model = lgb.train(params, lgbtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features =   (config.feature_dict['trade_detail_feature']+\n",
    "              config.feature_dict['recent_login_detail']+\n",
    "              config.feature_dict['trade_and_recent_login_comparing']+\n",
    "              config.feature_dict['login_trade_hist_stats'])\n",
    "feature_sequence_list = []\n",
    "for feature in features:\n",
    "    feature_sequence_list.append(pd.read_pickle(feature_path+feature+'.pkl').reshape(-1,1))\n",
    "    \n",
    "trade_tt_mat = np.hstack(feature_sequence_list)\n",
    "#trade_tt_mat[trade_tt_mat==-10]=np.nan\n",
    "\n",
    "validation_tuple_list = config.single_module_validation_indice_set\n",
    "train_labels = pd.read_pickle(data_path+'trade_train_label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's auc: 0.93919\ttrain's fbeta_score_01: 0.923553\ttest's auc: 0.922661\ttest's fbeta_score_01: 0.831276\n",
      "[200]\ttrain's auc: 0.97702\ttrain's fbeta_score_01: 0.937239\ttest's auc: 0.959978\ttest's fbeta_score_01: 0.833181\n",
      "[300]\ttrain's auc: 0.992494\ttrain's fbeta_score_01: 0.942157\ttest's auc: 0.967505\ttest's fbeta_score_01: 0.823244\n",
      "[400]\ttrain's auc: 0.997036\ttrain's fbeta_score_01: 0.954429\ttest's auc: 0.967873\ttest's fbeta_score_01: 0.823529\n",
      "[500]\ttrain's auc: 0.998632\ttrain's fbeta_score_01: 0.969252\ttest's auc: 0.967099\ttest's fbeta_score_01: 0.821403\n",
      "[600]\ttrain's auc: 0.999521\ttrain's fbeta_score_01: 0.983373\ttest's auc: 0.966179\ttest's fbeta_score_01: 0.798706\n",
      "f_beta score for the turn 1 is 0.79870596693\n",
      "[100]\ttrain's auc: 0.95801\ttrain's fbeta_score_01: 0.932748\ttest's auc: 0.846613\ttest's fbeta_score_01: 0.118475\n",
      "[200]\ttrain's auc: 0.983573\ttrain's fbeta_score_01: 0.943383\ttest's auc: 0.886538\ttest's fbeta_score_01: 0.352618\n",
      "[300]\ttrain's auc: 0.99384\ttrain's fbeta_score_01: 0.948722\ttest's auc: 0.901938\ttest's fbeta_score_01: 0.409672\n",
      "[400]\ttrain's auc: 0.998148\ttrain's fbeta_score_01: 0.960891\ttest's auc: 0.909854\ttest's fbeta_score_01: 0.387964\n",
      "[500]\ttrain's auc: 0.999164\ttrain's fbeta_score_01: 0.969659\ttest's auc: 0.912912\ttest's fbeta_score_01: 0.366856\n",
      "[600]\ttrain's auc: 0.999657\ttrain's fbeta_score_01: 0.978471\ttest's auc: 0.912345\ttest's fbeta_score_01: 0.384304\n",
      "f_beta score for the turn 2 is 0.384304399524\n",
      "[100]\ttrain's auc: 0.949772\ttrain's fbeta_score_01: 0.9439\ttest's auc: 0.919627\ttest's fbeta_score_01: 0.808375\n",
      "[200]\ttrain's auc: 0.980322\ttrain's fbeta_score_01: 0.944936\ttest's auc: 0.941927\ttest's fbeta_score_01: 0.815625\n",
      "[300]\ttrain's auc: 0.99334\ttrain's fbeta_score_01: 0.953939\ttest's auc: 0.949981\ttest's fbeta_score_01: 0.81499\n",
      "[400]\ttrain's auc: 0.99813\ttrain's fbeta_score_01: 0.964808\ttest's auc: 0.954617\ttest's fbeta_score_01: 0.793963\n",
      "[500]\ttrain's auc: 0.999317\ttrain's fbeta_score_01: 0.970184\ttest's auc: 0.953799\ttest's fbeta_score_01: 0.783427\n",
      "[600]\ttrain's auc: 0.999714\ttrain's fbeta_score_01: 0.984769\ttest's auc: 0.953105\ttest's fbeta_score_01: 0.777456\n",
      "f_beta score for the turn 3 is 0.777455986586\n",
      "[100]\ttrain's auc: 0.951951\ttrain's fbeta_score_01: 0.935491\ttest's auc: 0.881428\ttest's fbeta_score_01: 0.248768\n",
      "[200]\ttrain's auc: 0.975765\ttrain's fbeta_score_01: 0.94598\ttest's auc: 0.922981\ttest's fbeta_score_01: 0.40159\n",
      "[300]\ttrain's auc: 0.993464\ttrain's fbeta_score_01: 0.951718\ttest's auc: 0.936066\ttest's fbeta_score_01: 0.40878\n",
      "[400]\ttrain's auc: 0.998108\ttrain's fbeta_score_01: 0.962105\ttest's auc: 0.939566\ttest's fbeta_score_01: 0.396437\n",
      "[500]\ttrain's auc: 0.999176\ttrain's fbeta_score_01: 0.972302\ttest's auc: 0.942799\ttest's fbeta_score_01: 0.408136\n",
      "[600]\ttrain's auc: 0.99972\ttrain's fbeta_score_01: 0.986889\ttest's auc: 0.941171\ttest's fbeta_score_01: 0.402792\n",
      "f_beta score for the turn 4 is 0.402791625125\n",
      "[100]\ttrain's auc: 0.924723\ttrain's fbeta_score_01: 0.926893\ttest's auc: 0.914984\ttest's fbeta_score_01: 0.764106\n",
      "[200]\ttrain's auc: 0.977686\ttrain's fbeta_score_01: 0.943093\ttest's auc: 0.949051\ttest's fbeta_score_01: 0.786388\n",
      "[300]\ttrain's auc: 0.992546\ttrain's fbeta_score_01: 0.942584\ttest's auc: 0.962595\ttest's fbeta_score_01: 0.786325\n",
      "[400]\ttrain's auc: 0.998045\ttrain's fbeta_score_01: 0.951331\ttest's auc: 0.962906\ttest's fbeta_score_01: 0.781643\n",
      "[500]\ttrain's auc: 0.999263\ttrain's fbeta_score_01: 0.963145\ttest's auc: 0.962373\ttest's fbeta_score_01: 0.779726\n",
      "[600]\ttrain's auc: 0.999689\ttrain's fbeta_score_01: 0.977966\ttest's auc: 0.961072\ttest's fbeta_score_01: 0.786325\n",
      "f_beta score for the turn 5 is 0.786324786325\n",
      "[100]\ttrain's auc: 0.955394\ttrain's fbeta_score_01: 0.920691\ttest's auc: 0.878846\ttest's fbeta_score_01: 0.38727\n",
      "[200]\ttrain's auc: 0.978893\ttrain's fbeta_score_01: 0.933002\ttest's auc: 0.908099\ttest's fbeta_score_01: 0.526042\n",
      "[300]\ttrain's auc: 0.993762\ttrain's fbeta_score_01: 0.945386\ttest's auc: 0.930404\ttest's fbeta_score_01: 0.537949\n",
      "[400]\ttrain's auc: 0.99786\ttrain's fbeta_score_01: 0.960972\ttest's auc: 0.936121\ttest's fbeta_score_01: 0.563749\n",
      "[500]\ttrain's auc: 0.998938\ttrain's fbeta_score_01: 0.979428\ttest's auc: 0.937826\ttest's fbeta_score_01: 0.605308\n",
      "[600]\ttrain's auc: 0.999526\ttrain's fbeta_score_01: 0.984045\ttest's auc: 0.938594\ttest's fbeta_score_01: 0.588513\n",
      "f_beta score for the turn 6 is 0.588512763596\n",
      "The mean of the cv_scores is: 0.623015921348\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "cv_result = []\n",
    "models = []\n",
    "i = 0\n",
    "\n",
    "for train_indice,val_indice in validation_tuple_list:\n",
    "    #print trade_train_val.iloc[train_indice]['month'].unique(),trade_train_val.iloc[val_indice]['month'].unique()\n",
    "    #print trade_train_val.iloc[train_indice].shape,trade_train_val.iloc[val_indice].shape\n",
    "    result_dict = {}\n",
    "    \n",
    "    #filter the features\n",
    "    dev_X, val_X = trade_tt_mat[train_indice], trade_tt_mat[val_indice]\n",
    "    dev_y, val_y = train_labels.iloc[train_indice].values, train_labels.iloc[val_indice].values\n",
    "\n",
    "    \n",
    "    preds, model = runLGBM(dev_X, dev_y, val_X, val_y,feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=600,watch_dict=result_dict,feval = f_beta_01)\n",
    "\n",
    "    #result_f_beta = f_beta_01(val_y.values, preds>0.5)\n",
    "    result_f_beta  = fbeta_score( val_y,preds > 0.5, 0.1)\n",
    "    \n",
    "    cv_scores.append(result_f_beta)\n",
    "    cv_result.append(result_dict)\n",
    "    models.append(model)\n",
    "    i+=1\n",
    "    print('f_beta score for the turn '+str(i)+' is '+str(result_f_beta))\n",
    "\n",
    "print('The mean of the cv_scores is:',np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finding the best iteration\n",
    "pd_list = []\n",
    "for dic in cv_result:\n",
    "    pd_list.append(pd.DataFrame(dic['test']))\n",
    "for i in range(len(pd_list)):\n",
    "    pd_list[i].columns = pd_list[i].columns+'_'+str(i)\n",
    "validation_result = pd.concat(pd_list,axis = 1)\n",
    "validation_result['auc_avg'] = validation_result.apply(lambda x : np.mean([x.auc_0,x.auc_1,x.auc_2,x.auc_3,x.auc_4]),axis = 1)\n",
    "validation_result['fbeta_avg'] = validation_result.apply(lambda x : np.mean([x.fbeta_score_01_0,x.fbeta_score_01_1,\n",
    "                                                                     x.fbeta_score_01_2,x.fbeta_score_01_3,\n",
    "                                                                     x.fbeta_score_01_4]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "print(validation_result['auc_avg'].idxmax())\n",
    "print(validation_result['fbeta_avg'].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "train_X = trade_tt_mat[config.train_2_6_index]\n",
    "test_X = trade_tt_mat[config.trade_train_size:]\n",
    "train_y = train_labels[config.train_2_6_index]\n",
    "\n",
    "preds, _ = runLGBM(train_X, train_y, test_X, feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=488,watch_dict=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17875, 2)\n"
     ]
    }
   ],
   "source": [
    "result_path = '../../kaggleData/JD_logging/result/'\n",
    "test_rowkey = pd.read_pickle(data_path+'trade_test_rowkey.pkl')\n",
    "pred_label = pd.Series(preds > 0.5)\n",
    "result_set = pd.DataFrame(test_rowkey)\n",
    "result_set['is_risk'] = pred_label.astype(int)\n",
    "\n",
    "print(result_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(pred_label,result_path+'adding_type_C_488.pkl')\n",
    "result_set.to_csv(result_path+'adding_type_C_488.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17875, 2)\n"
     ]
    }
   ],
   "source": [
    "train_X = trade_tt_mat[config.train_2_6_index]\n",
    "test_X = trade_tt_mat[config.trade_train_size:]\n",
    "train_y = train_labels[config.train_2_6_index]\n",
    "\n",
    "preds, _ = runLGBM(train_X, train_y, test_X, feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=288,watch_dict=None)\n",
    "\n",
    "result_path = '../../kaggleData/JD_logging/result/'\n",
    "test_rowkey = pd.read_pickle(data_path+'trade_test_rowkey.pkl')\n",
    "pred_label = pd.Series(preds > 0.5)\n",
    "result_set = pd.DataFrame(test_rowkey)\n",
    "result_set['is_risk'] = pred_label.astype(int)\n",
    "\n",
    "print result_set.shape\n",
    "\n",
    "pd.to_pickle(pred_label,result_path+'adding_type_C_482turn.pkl')\n",
    "result_set.to_csv(result_path+'adding_type_C_482turn.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
